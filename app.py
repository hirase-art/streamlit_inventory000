import streamlit as st
import pandas as pd
import logging
import glob # â˜… ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’æ‰±ã†ãŸã‚ã«globã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.pyplot as plt # â˜… ã‚°ãƒ©ãƒ•ä½œæˆã®ãŸã‚ã«pyplotã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import japanize_matplotlib # æ—¥æœ¬èªæ–‡å­—åŒ–ã‘å¯¾ç­–
import numpy as np # â˜… æ•°å€¤è¨ˆç®—ã®ãŸã‚ã«numpyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.gridspec as gridspec

st.write(st.secrets) # secretsãŒèª­ã¿è¾¼ã‚ã¦ã„ã‚Œã°ã€ä¸­èº«ãŒè¡¨ç¤ºã•ã‚Œã¾ã™

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='app.log',
    filemode='w'
)

logging.info("--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---")
# --------------------------------------------------------------------------
# 1. Supabase æ¥ç¶šè¨­å®š (æ–°è¦è¿½åŠ )
# --------------------------------------------------------------------------
# .streamlit/secrets.toml ã«è¨­å®šã—ãŸæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶šã—ã¾ã™
try:
    conn = st.connection("postgresql", type="sql")
except Exception as e:
    st.error(f"Supabaseã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚secrets.tomlã‚’ç¢ºèªã—ã¦ãã ã•ã„: {e}")

# --------------------------------------------------------------------------
# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data
def load_single_csv(path, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰å˜ä¸€ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    logging.info(f"load_single_csv: {path} ã‚’ {encoding} ã¨ã—ã¦èª­ã¿è¾¼ã¿é–‹å§‹ã€‚")
    try:
        # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
        df = pd.read_csv(path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str, 'æ¥­å‹™åŒºåˆ†ID': str, 'SET_ID': str})
        logging.info(f"load_single_csv: {path} ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return df
    except FileNotFoundError:
        logging.error(f"load_single_csv: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
        st.error(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ•ã‚¡ã‚¤ãƒ«ãŒã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„: {path}")
        return None
    except Exception as e:
        logging.error(f"load_single_csv: {path} èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data
def load_multiple_csv(pattern, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹è¤‡æ•°ã®CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆã™ã‚‹é–¢æ•°"""
    logging.info(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã®æ¤œç´¢ã‚’é–‹å§‹ã€‚")
    
    file_list = glob.glob(pattern)
    
    if not file_list:
        logging.warning(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
        st.warning(f"è­¦å‘Š: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {pattern}")
        return None

    logging.info(f"{len(file_list)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {file_list}")
    
    df_list = []
    for file_path in file_list:
        try:
            # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
            df = pd.read_csv(file_path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str})
            df_list.append(df)
        except Exception as e:
            logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {file_path}, ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            continue 
            
    if not df_list:
        logging.error("èª­ã¿è¾¼ã‚ãŸãƒ‡ãƒ¼ã‚¿ãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    logging.info("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã—ã¾ã™ã€‚")
    combined_df = pd.concat(df_list, ignore_index=True)
    return combined_df

# æ£’ã‚°ãƒ©ãƒ•ã«æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°
def add_labels_to_stacked_bar(ax, data_df):
    """ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ã®å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹"""
    bottom = pd.Series([0.0] * len(data_df), index=data_df.index) 
    x_positions = np.arange(len(data_df.index)) 

    for col in data_df.columns:
        values = data_df[col]
        # å€¤ãŒä¸€å®šä»¥ä¸Šã®å ´åˆã®ã¿ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºï¼ˆé–¾å€¤ã‚’èª¿æ•´ï¼‰
        threshold = values.sum() * 0.03 
        non_zero_values = values[values > threshold] 
        
        valid_indices = non_zero_values.index
        # y_pos ã®è¨ˆç®—æ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã™ã‚‹ã‚ˆã†ã«èª¿æ•´ (NaNã‚’é¿ã‘ã‚‹)
        y_pos = bottom.loc[valid_indices].fillna(0) + non_zero_values.fillna(0) / 2
        
        x_pos_map = {label: i for i, label in enumerate(data_df.index)}
        valid_x_positions = [x_pos_map.get(idx) for idx in valid_indices if x_pos_map.get(idx) is not None]

        valid_non_zero_values = non_zero_values.iloc[:len(valid_x_positions)]
        valid_y_pos = y_pos.iloc[:len(valid_x_positions)]

        for i, val in enumerate(valid_non_zero_values):
             if i < len(valid_x_positions):
                # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºå¾®èª¿æ•´
                ax.text(valid_x_positions[i], valid_y_pos.iloc[i], f'{int(val)}', 
                        ha='center', va='center', fontsize=5, color='white', fontweight='bold') 
            
        bottom += values.fillna(0) 

# â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… CSVå¤‰æ›ç”¨é–¢æ•°
@st.cache_data
def convert_df(df):
    # utf-8-sigã«ã™ã‚‹ã“ã¨ã§Excelã§é–‹ã„ãŸæ™‚ã®æ–‡å­—åŒ–ã‘ã‚’é˜²ã
    return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')

try:
    st.set_page_config(layout="wide") 
    st.title('ğŸ“Š åœ¨åº«ãƒ»å‡ºè·ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚¢ãƒ—ãƒª')

    # --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    DATA_PATH1 = "T_9x30.csv"
    DATA_PATH_PACK_MASTER = "PACK_Classification.csv"
    DATA_PATH_SET_MASTER = "SET_Class.csv"
    DATA_PATH3_PATTERN = "CZ04003_*.csv"
    DATA_PATH5 = "T_9x07.csv"

    df1 = load_single_csv(DATA_PATH1, encoding='utf-8')
    df_pack_master = load_single_csv(DATA_PATH_PACK_MASTER, encoding='utf-8') 
    df_set_master = load_single_csv(DATA_PATH_SET_MASTER, encoding='utf-8') 
    df3 = load_multiple_csv(DATA_PATH3_PATTERN, encoding='cp932')
    df5 = load_single_csv(DATA_PATH5, encoding='utf-8')


    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®šç¾© ---
    base_df_monthly = pd.DataFrame()
    base_df_weekly = pd.DataFrame()
    selected_daibunrui_shipping = [] 
    selected_shobunrui_shipping = []
    product_name_search_shipping = "" 
    selected_product_ids_shipping = []
    selected_gyomu = "ã™ã¹ã¦"
    selected_soko_shipping = "ã™ã¹ã¦"
    gyomu_display_map = {'4': 'å¸å‡ºè·æ©Ÿèƒ½', '7': 'é€šè²©å‡ºè·æ©Ÿèƒ½'}
    soko_display_map = {'7': 'å¤§é˜ª', '8': 'åƒè‘‰'}
    num_months = 12 
    num_weeks = 12 
    aggregation_level = "å•†å“ID" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    show_total_column = "ãªã—" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    # --- å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ã®æº–å‚™ ---
    if df1 is not None and df_pack_master is not None and df_set_master is not None:
        
        st.sidebar.header(":blue[å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")

        # 1. é›†è¨ˆå˜ä½ã®é¸æŠ
        unit_selection = st.sidebar.radio("é›†è¨ˆå˜ä½:", ["Pack", "SET"], horizontal=True, key='unit_selection')

        # 2. ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åˆ‡ã‚Šæ›¿ãˆã¨æ•´å½¢
        if unit_selection == "Pack":
            df_master_for_shipping = df_pack_master.copy()
            master_cols = ['å•†å“ID', 'å•†å“å', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        else: # SETã®å ´åˆ
            df_master_for_shipping = df_set_master.copy()
            df_master_for_shipping = df_master_for_shipping.rename(columns={'SET_ID': 'å•†å“ID', 'ã‚»ãƒƒãƒˆæ§‹æˆåç§°': 'å•†å“å'})
            master_cols = ['å•†å“ID', 'å•†å“å', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        
        # 3. ãƒãƒ¼ã‚¸å‡¦ç†ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if all(col in df_master_for_shipping.columns for col in master_cols):
            df_master_shipping = df_master_for_shipping[master_cols].drop_duplicates(subset='å•†å“ID')
            df1['å•†å“ID'] = df1['å•†å“ID'].astype(str)
            df_master_shipping['å•†å“ID'] = df_master_shipping['å•†å“ID'].astype(str)
            base_df_monthly = pd.merge(df1, df_master_shipping, on='å•†å“ID', how='left')
            
            if df5 is not None:
                df5['å•†å“ID'] = df5['å•†å“ID'].astype(str)
                base_df_weekly = pd.merge(df5, df_master_shipping, on='å•†å“ID', how='left')

            # --- ãƒ•ã‚£ãƒ«ã‚¿UI ---
            st.sidebar.markdown("---")
            aggregation_level = st.sidebar.radio("é›†è¨ˆç²’åº¦:", ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID"], index=3, horizontal=True, key='agg_level')
            show_total_column = st.sidebar.radio("åˆè¨ˆè¡¨ç¤º:", ["ãªã—", "ã‚ã‚Š"], horizontal=True, key='show_total')
            st.sidebar.markdown("---")

            if 'å¤§åˆ†é¡' in base_df_monthly.columns:
                daibunrui_options = base_df_monthly['å¤§åˆ†é¡'].dropna().unique().tolist()
                daibunrui_options.sort()
                selected_daibunrui_shipping = st.sidebar.multiselect(
                    "å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=daibunrui_options, key='daibunrui_shipping'
                )
            
            df_after_daibunrui_filter = base_df_monthly[base_df_monthly['å¤§åˆ†é¡'].isin(selected_daibunrui_shipping)] if selected_daibunrui_shipping else base_df_monthly

            if 'å°åˆ†é¡' in df_after_daibunrui_filter.columns:
                shobunrui_options = df_after_daibunrui_filter['å°åˆ†é¡'].dropna().unique().tolist()
                shobunrui_options.sort()
                selected_shobunrui_shipping = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options, key='shobunrui_shipping')
            
            df_after_shobunrui_filter = df_after_daibunrui_filter[df_after_daibunrui_filter['å°åˆ†é¡'].isin(selected_shobunrui_shipping)] if selected_shobunrui_shipping else df_after_daibunrui_filter
            
            product_name_search_shipping = st.sidebar.text_input("å•†å“åã§ã‚ã„ã¾ã„æ¤œç´¢:", key='product_name_shipping').strip()
            df_after_name_filter = df_after_shobunrui_filter[df_after_shobunrui_filter['å•†å“å'].str.contains(product_name_search_shipping, na=False)] if product_name_search_shipping else df_after_shobunrui_filter
            
            product_ids_input_shipping = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_shipping').strip()
            selected_product_ids_shipping = [pid.strip() for pid in product_ids_input_shipping.split(',')] if product_ids_input_shipping else []
            df_after_product_id_filter = df_after_name_filter[df_after_name_filter['å•†å“ID'].isin(selected_product_ids_shipping)] if selected_product_ids_shipping else df_after_name_filter

            if 'æ¥­å‹™åŒºåˆ†ID' in df_after_product_id_filter.columns:
                gyomu_options = df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'].dropna().unique().tolist()
                gyomu_options.sort()
                gyomu_options.insert(0, "ã™ã¹ã¦")
                selected_gyomu = st.sidebar.radio("æ¥­å‹™åŒºåˆ†IDã§çµã‚Šè¾¼ã¿:", options=gyomu_options, key='gyomu_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else gyomu_display_map.get(x, x))
            
            df_after_gyomu_filter = df_after_product_id_filter[df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu] if selected_gyomu != "ã™ã¹ã¦" else df_after_product_id_filter
            
            if 'å€‰åº«ID' in df_after_gyomu_filter.columns:
                soko_options = df_after_gyomu_filter['å€‰åº«ID'].dropna().unique().tolist()
                soko_options.sort()
                soko_options.insert(0, "ã™ã¹ã¦")
                selected_soko_shipping = st.sidebar.radio("å€‰åº«IDã§çµã‚Šè¾¼ã¿:", options=soko_options, key='soko_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else soko_display_map.get(x, x))
            
            st.sidebar.markdown("---")
            num_months = st.sidebar.slider("æœˆé–“è¡¨ç¤ºæœŸé–“ï¼ˆãƒ¶æœˆï¼‰", min_value=3, max_value=26, value=12, key='num_months')
            num_weeks = st.sidebar.slider("é€±é–“è¡¨ç¤ºæœŸé–“ï¼ˆé€±ï¼‰", min_value=3, max_value=15, value=12, key='num_weeks')
        else:
            st.error(f"é¸æŠã•ã‚ŒãŸãƒã‚¹ã‚¿({unit_selection})ã«å¿…è¦ãªåˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")


    # --- åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ã®æº–å‚™ ---
    base_df_stock = pd.DataFrame()
    selected_daibunrui_stock = []
    selected_shobunrui_stock = []
    product_name_search_stock = ""
    selected_product_ids_stock = []
    selected_quality_stock = "ã™ã¹ã¦"

    if df3 is not None and df_pack_master is not None:
        master_cols_stock = ['å•†å“ID', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡', 'å•†å“å'] 
        if all(col in df_pack_master.columns for col in master_cols_stock):
            cols_to_drop = ['å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡', 'å•†å“å'] 
            df3['å•†å“ID'] = df3['å•†å“ID'].astype(str)
            df_master_stock = df_pack_master[master_cols_stock].drop_duplicates(subset='å•†å“ID')
            df_master_stock['å•†å“ID'] = df_master_stock['å•†å“ID'].astype(str)
            df3_for_merge = df3.drop(columns=cols_to_drop, errors='ignore')
            base_df_stock = pd.merge(df3_for_merge, df_master_stock, on='å•†å“ID', how='left')
        
        st.sidebar.header(":blue[åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
        if 'å¤§åˆ†é¡' in base_df_stock.columns:
            daibunrui_options_stock = base_df_stock['å¤§åˆ†é¡'].dropna().unique().tolist()
            daibunrui_options_stock.sort()
            selected_daibunrui_stock = st.sidebar.multiselect(
                "å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=daibunrui_options_stock, key='daibunrui_stock'
            )
        
        df_after_daibunrui_filter_stock = base_df_stock[base_df_stock['å¤§åˆ†é¡'].isin(selected_daibunrui_stock)] if selected_daibunrui_stock else base_df_stock
        
        if 'å°åˆ†é¡' in df_after_daibunrui_filter_stock.columns:
            shobunrui_options_stock = df_after_daibunrui_filter_stock['å°åˆ†é¡'].dropna().unique().tolist()
            shobunrui_options_stock.sort()
            selected_shobunrui_stock = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options_stock, key='shobunrui_stock')
        
        df_after_shobunrui_filter_stock = df_after_daibunrui_filter_stock[df_after_daibunrui_filter_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock)] if selected_shobunrui_stock else df_after_daibunrui_filter_stock
        product_name_search_stock = st.sidebar.text_input("å•†å“åã§ã‚ã„ã¾ã„æ¤œç´¢:", key='product_name_stock').strip()
        df_after_name_filter_stock = df_after_shobunrui_filter_stock[df_after_shobunrui_filter_stock['å•†å“å'].str.contains(product_name_search_stock, na=False)] if product_name_search_stock else df_after_shobunrui_filter_stock
        product_ids_input_stock = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_stock').strip()
        selected_product_ids_stock = [pid.strip() for pid in product_ids_input_stock.split(',')] if product_ids_input_stock else []
        df_after_product_id_filter_stock = df_after_name_filter_stock[df_after_name_filter_stock['å•†å“ID'].isin(selected_product_ids_stock)] if selected_product_ids_stock else df_after_name_filter_stock
        
        if 'å“è³ªåŒºåˆ†å' in df_after_product_id_filter_stock.columns:
            quality_options_stock = df_after_product_id_filter_stock['å“è³ªåŒºåˆ†å'].dropna().unique().tolist()
            quality_options_stock.insert(0, "ã™ã¹ã¦")
            selected_quality_stock = st.sidebar.radio("å“è³ªåŒºåˆ†åã§çµã‚Šè¾¼ã¿:", options=quality_options_stock, key='quality_stock')


    # --- ã‚¿ãƒ–ã®ä½œæˆ ---
    tab_shipping, tab_stock = st.tabs(["ğŸ“ å‡ºè·æƒ…å ±", "ğŸ“Š åœ¨åº«æƒ…å ±"])

    # --- å‡ºè·æƒ…å ±ã®ã‚¿ãƒ– ---
    with tab_shipping:
        st.header("ğŸšš å‡ºè·æƒ…å ±")
        if not base_df_monthly.empty:
            # æœˆé–“å‡ºè·
            st.markdown("---")
            st.subheader("æœˆé–“å‡ºè·æ•°")
            gyomu_display_str = "ã™ã¹ã¦" if selected_gyomu == "ã™ã¹ã¦" else gyomu_display_map.get(selected_gyomu, selected_gyomu)
            soko_display_str = "ã™ã¹ã¦" if selected_soko_shipping == "ã™ã¹ã¦" else soko_display_map.get(selected_soko_shipping, selected_soko_shipping)
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping if selected_daibunrui_shipping else 'ã™ã¹ã¦'}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“å:** `{product_name_search_shipping if product_name_search_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
            
            df_monthly_filtered = base_df_monthly[
                (base_df_monthly['å¤§åˆ†é¡'].isin(selected_daibunrui_shipping) if selected_daibunrui_shipping else True) &
                (base_df_monthly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                (base_df_monthly['å•†å“å'].str.contains(product_name_search_shipping, na=False) if product_name_search_shipping else True) &
                (base_df_monthly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                (base_df_monthly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                (base_df_monthly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
            ]
            
            required_cols = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "month_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
            if not df_monthly_filtered.empty and all(col in df_monthly_filtered.columns for col in required_cols):
                
                # é›†è¨ˆç²’åº¦ã«å¿œã˜ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆ
                if aggregation_level == "å¤§åˆ†é¡":
                    index_cols = ["å¤§åˆ†é¡"]
                    graph_stack_col = "å¤§åˆ†é¡"
                elif aggregation_level == "ä¸­åˆ†é¡":
                    index_cols = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡"]
                    graph_stack_col = "ä¸­åˆ†é¡"
                elif aggregation_level == "å°åˆ†é¡":
                    index_cols = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
                    graph_stack_col = "å°åˆ†é¡"
                else: # å•†å“ID
                    index_cols = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"]
                    graph_stack_col = "å•†å“å"

                pivot = df_monthly_filtered.pivot_table(index=index_cols, columns="month_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                recent_cols = pivot.columns[-num_months:] 
                pivot_filtered = pivot[pivot[recent_cols].sum(axis=1) != 0]
                pivot_display = pivot_filtered.loc[:, recent_cols].copy() 

                # åˆè¨ˆåˆ—ã¨åˆè¨ˆè¡Œã‚’è¿½åŠ 
                if show_total_column == "ã‚ã‚Š":
                    # æ¨ªè¨ˆï¼ˆåˆ—ã®åˆè¨ˆï¼‰ã‚’è¿½åŠ 
                    pivot_display['åˆè¨ˆ'] = pivot_display.sum(axis=1)
                    # ç¸¦è¨ˆï¼ˆè¡Œã®åˆè¨ˆï¼‰ã‚’è¿½åŠ 
                    total_row = pivot_display.sum()
                    total_row.name = ('åˆè¨ˆ',) * len(pivot_display.index.names) 
                    pivot_display = pd.concat([pivot_display, total_row.to_frame().T]) 
                    pivot_display.index.names = index_cols

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.info(f"ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘{num_months}ãƒ¶æœˆåˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                    st.dataframe(pivot_display.reset_index(), height=400, use_container_width=True)
                    # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                    csv = convert_df(pivot_display)
                    st.download_button(
                        label="ãƒ‡ãƒ¼ã‚¿ã‚’CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name=f'monthly_shipping_data_{num_months}months.csv',
                        mime='text/csv',
                    )

                with col2:
                    st.write(f"ã‚°ãƒ©ãƒ•ï¼ˆ{graph_stack_col}åˆ¥ç©ã¿ä¸Šã’ï¼‰") 
                    # ã‚°ãƒ©ãƒ•ã‚‚å‹•çš„ã«é›†è¨ˆå¯¾è±¡ã‚’å¤‰æ›´
                    chart_df_monthly_base = df_monthly_filtered.pivot_table(index='month_code', columns=graph_stack_col, values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                    chart_df_monthly_display = chart_df_monthly_base.iloc[-num_months:, :] 
                    if not chart_df_monthly_display.empty:
                        chart_data_top = chart_df_monthly_display 
                        fig = plt.figure() 
                        gs = gridspec.GridSpec(5, 1, height_ratios=[3, 1, 0.1, 0.1, 0.1]) 
                        ax_chart = fig.add_subplot(gs[0]) 
                        ax_legend = fig.add_subplot(gs[1]) 
                        ax_legend.axis('off') 
                        ax_chart.set_facecolor('lightgray') 
                        chart_data_top.plot(kind='bar', stacked=True, ax=ax_chart, legend=False) 
                        try: add_labels_to_stacked_bar(ax_chart, chart_data_top)
                        except Exception as label_e: st.caption("æ•°å€¤ãƒ©ãƒ™ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼")
                        ax_chart.set_xlabel("æœˆã‚³ãƒ¼ãƒ‰") 
                        ax_chart.set_ylabel("åˆè¨ˆå‡ºè·æ•°")
                        tick_interval = max(1, len(chart_data_top) // 10) 
                        ax_chart.set_xticks(np.arange(0, len(chart_data_top), tick_interval))
                        ax_chart.set_xticklabels(chart_data_top.index[::tick_interval], rotation=45, ha='right', fontsize=8) 
                        handles, labels = ax_chart.get_legend_handles_labels()
                        ncol_legend = min(5, len(labels)) 
                        ax_legend.legend(handles, labels, title=graph_stack_col, loc='upper center', ncol=ncol_legend, fontsize=6) 
                        plt.tight_layout(rect=[0, 0.05, 1, 1]) 
                        st.pyplot(fig, use_container_width=True)
                    else: st.warning("æœˆé–“ã‚°ãƒ©ãƒ•: ãƒ‡ãƒ¼ã‚¿ç„¡")
            else:
                st.warning("æœˆé–“å‡ºè·: æ¡ä»¶ä¸€è‡´ãƒ‡ãƒ¼ã‚¿ç„¡ or åˆ—ä¸è¶³")

            # é€±é–“å‡ºè·
            if not base_df_weekly.empty:
                st.markdown("---")
                st.subheader("é€±é–“å‡ºè·æ•°")
                st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping if selected_daibunrui_shipping else 'ã™ã¹ã¦'}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“å:** `{product_name_search_shipping if product_name_search_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
                
                df_weekly_filtered = base_df_weekly[
                    (base_df_weekly['å¤§åˆ†é¡'].isin(selected_daibunrui_shipping) if selected_daibunrui_shipping else True) &
                    (base_df_weekly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                    (base_df_weekly['å•†å“å'].str.contains(product_name_search_shipping, na=False) if product_name_search_shipping else True) &
                    (base_df_weekly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                    (base_df_weekly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                    (base_df_weekly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
                ]
                
                required_cols_weekly = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "week_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
                if not df_weekly_filtered.empty and all(col in df_weekly_filtered.columns for col in required_cols_weekly):
                    
                    # é›†è¨ˆç²’åº¦ã«å¿œã˜ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆï¼ˆé€±é–“ï¼‰
                    if aggregation_level == "å¤§åˆ†é¡":
                        index_cols_w = ["å¤§åˆ†é¡"]
                        graph_stack_col_w = "å¤§åˆ†é¡"
                    elif aggregation_level == "ä¸­åˆ†é¡":
                        index_cols_w = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡"]
                        graph_stack_col_w = "ä¸­åˆ†é¡"
                    elif aggregation_level == "å°åˆ†é¡":
                        index_cols_w = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
                        graph_stack_col_w = "å°åˆ†é¡"
                    else: # å•†å“ID
                        index_cols_w = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"]
                        graph_stack_col_w = "å•†å“å"

                    pivot_weekly = df_weekly_filtered.pivot_table(index=index_cols_w, columns="week_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                    recent_cols_weekly = pivot_weekly.columns[-num_weeks:]
                    pivot_weekly_filtered = pivot_weekly[pivot_weekly[recent_cols_weekly].sum(axis=1) != 0]
                    pivot_weekly_display = pivot_weekly_filtered.loc[:, recent_cols_weekly].copy()

                    # åˆè¨ˆåˆ—ã¨åˆè¨ˆè¡Œã‚’è¿½åŠ ï¼ˆé€±é–“ï¼‰
                    if show_total_column == "ã‚ã‚Š":
                        # æ¨ªè¨ˆ
                        pivot_weekly_display['åˆè¨ˆ'] = pivot_weekly_display.sum(axis=1)
                        # ç¸¦è¨ˆ
                        total_row_w = pivot_weekly_display.sum()
                        total_row_w.name = ('åˆè¨ˆ',) * len(pivot_weekly_display.index.names)
                        pivot_weekly_display = pd.concat([pivot_weekly_display, total_row_w.to_frame().T])
                        pivot_weekly_display.index.names = index_cols_w

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info(f"ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘{num_weeks}é€±åˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_weekly_display.reset_index(), height=400, use_container_width=True)
                        # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                        csv_weekly = convert_df(pivot_weekly_display)
                        st.download_button(
                            label="ãƒ‡ãƒ¼ã‚¿ã‚’CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_weekly,
                            file_name=f'weekly_shipping_data_{num_weeks}weeks.csv',
                            mime='text/csv',
                        )
                    with col2:
                        st.write(f"ã‚°ãƒ©ãƒ•ï¼ˆ{graph_stack_col_w}åˆ¥ç©ã¿ä¸Šã’ï¼‰") 
                        chart_df_weekly_base = df_weekly_filtered.pivot_table(index='week_code', columns=graph_stack_col_w, values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                        chart_df_weekly_display = chart_df_weekly_base.iloc[-num_weeks:, :] 
                        if not chart_df_weekly_display.empty:
                            chart_data_top_w = chart_df_weekly_display 
                            fig_w = plt.figure()
                            gs_w = gridspec.GridSpec(5, 1, height_ratios=[3, 1, 0.1, 0.1, 0.1]) 
                            ax_chart_w = fig_w.add_subplot(gs_w[0])
                            ax_legend_w = fig_w.add_subplot(gs_w[1])
                            ax_legend_w.axis('off')
                            ax_chart_w.set_facecolor('lightgray')
                            chart_data_top_w.plot(kind='bar', stacked=True, ax=ax_chart_w, legend=False)
                            try: add_labels_to_stacked_bar(ax_chart_w, chart_data_top_w)
                            except Exception as label_e: st.caption("æ•°å€¤ãƒ©ãƒ™ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼")
                            ax_chart_w.set_xlabel("é€±ã‚³ãƒ¼ãƒ‰")
                            ax_chart_w.set_ylabel("åˆè¨ˆå‡ºè·æ•°")
                            tick_interval_w = max(1, len(chart_data_top_w) // 10) 
                            ax_chart_w.set_xticks(np.arange(0, len(chart_data_top_w), tick_interval_w))
                            ax_chart_w.set_xticklabels(chart_data_top_w.index[::tick_interval_w], rotation=45, ha='right', fontsize=8)
                            handles_w, labels_w = ax_chart_w.get_legend_handles_labels()
                            ncol_legend_w = min(5, len(labels_w))
                            ax_legend_w.legend(handles_w, labels_w, title=graph_stack_col_w, loc='upper center', ncol=ncol_legend_w, fontsize=6) 
                            plt.tight_layout(rect=[0, 0.05, 1, 1]) 
                            st.pyplot(fig_w, use_container_width=True) 
                        else: st.warning("é€±é–“ã‚°ãƒ©ãƒ•: ãƒ‡ãƒ¼ã‚¿ç„¡")
                else:
                    st.warning("é€±é–“å‡ºè·: æ¡ä»¶ä¸€è‡´ãƒ‡ãƒ¼ã‚¿ç„¡ or åˆ—ä¸è¶³")
    
    # --- åœ¨åº«æƒ…å ±ã®ã‚¿ãƒ– ---
    # (å¤‰æ›´ãªã—)
    with tab_stock:
        st.header("ğŸ“¦ åœ¨åº«æƒ…å ±")
        if not base_df_stock.empty:
            # â˜…â˜…â˜… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯æ›´æ–° (å¤§åˆ†é¡)
            pivot_target_df_stock = base_df_stock[
                (base_df_stock['å¤§åˆ†é¡'].isin(selected_daibunrui_stock) if selected_daibunrui_stock else True) &
                (base_df_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock) if selected_shobunrui_stock else True) &
                (base_df_stock['å•†å“å'].str.contains(product_name_search_stock, na=False) if product_name_search_stock else True) &
                (base_df_stock['å•†å“ID'].isin(selected_product_ids_stock) if selected_product_ids_stock else True) &
                (base_df_stock['å“è³ªåŒºåˆ†å'] == selected_quality_stock if selected_quality_stock != "ã™ã¹ã¦" else True)
            ]

            st.markdown("---")
            st.subheader("åˆ©ç”¨å¯èƒ½åœ¨åº«")
             # â˜…â˜…â˜… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¡¨ç¤ºæ›´æ–° (å¤§åˆ†é¡)
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_stock if selected_daibunrui_stock else 'ã™ã¹ã¦'}` | **å°åˆ†é¡:** `{selected_shobunrui_stock if selected_shobunrui_stock else 'ã™ã¹ã¦'}` | **å•†å“å:** `{product_name_search_stock if product_name_search_stock else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_stock if selected_product_ids_stock else 'ã™ã¹ã¦'}` | **å“è³ªåŒºåˆ†å:** `{selected_quality_stock}`")
            
            required_cols_stock = ["å•†å“ID", "å•†å“å", "å€‰åº«å", "åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)", "å¼•å½“æ•°"]
            if not pivot_target_df_stock.empty and all(col in pivot_target_df_stock.columns for col in required_cols_stock):
                try:
                    # (ä»¥é™ã€åœ¨åº«æƒ…å ±ã®è¡¨ç¤ºéƒ¨åˆ†ã¯å¤‰æ›´ãªã—)
                    pivot_target_df_stock['å®Ÿåœ¨åº«æ•°'] = pd.to_numeric(pivot_target_df_stock['åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)'], errors='coerce').fillna(0) - pd.to_numeric(pivot_target_df_stock['å¼•å½“æ•°'], errors='coerce').fillna(0)
                    pivot_index_stock = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"]
                    available_index_cols = [col for col in pivot_index_stock if col in pivot_target_df_stock.columns]
                    pivot_stock = pivot_target_df_stock.pivot_table(index=available_index_cols, columns="å€‰åº«å", values="å®Ÿåœ¨åº«æ•°", aggfunc="sum").fillna(0)
                    pivot_stock_filtered = pivot_stock[pivot_stock.sum(axis=1) != 0]

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info("ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯åˆè¨ˆåœ¨åº«ãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_stock_filtered.reset_index(), height=400, use_container_width=True)
                        # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                        csv_stock = convert_df(pivot_stock_filtered)
                        st.download_button(
                            label="ãƒ‡ãƒ¼ã‚¿ã‚’CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=csv_stock,
                            file_name='inventory_data.csv',
                            mime='text/csv',
                        )
                    with col2:
                        st.write("ã‚°ãƒ©ãƒ•ï¼ˆå¤§åˆ†é¡åˆ¥ åœ¨åº«æ§‹æˆæ¯”ï¼‰")
                        if 'å¤§åˆ†é¡' in pivot_target_df_stock.columns and 'å®Ÿåœ¨åº«æ•°' in pivot_target_df_stock.columns:
                            pie_data = pivot_target_df_stock.groupby('å¤§åˆ†é¡')['å®Ÿåœ¨åº«æ•°'].sum()
                            pie_data = pie_data[pie_data > 0] 
                            if not pie_data.empty:
                                fig, ax = plt.subplots() 
                                ax.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 8}) 
                                ax.axis('equal')
                                fig.patch.set_facecolor('lightgray')
                                st.pyplot(fig, use_container_width=True) 
                            else: st.warning("ã‚°ãƒ©ãƒ•åŒ–ã§ãã‚‹åœ¨åº«ãƒ‡ãƒ¼ã‚¿ç„¡")
                        else: st.warning("ã‚°ãƒ©ãƒ•ä½œæˆã«å¿…è¦ãªåˆ—ç„¡")
                except Exception as e:
                    st.error(f"ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("åœ¨åº«æƒ…å ±: æ¡ä»¶ä¸€è‡´ãƒ‡ãƒ¼ã‚¿ç„¡ or åˆ—ä¸è¶³")

    # --- å…±é€šã®ãƒ•ãƒƒã‚¿ãƒ¼ãªã© ---
    st.markdown("---")
    with st.expander("å–ã‚Šè¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä»•æ§˜ã«ã¤ã„ã¦"):
        st.markdown("""
        - **æœˆé–“å‡ºè·æƒ…å ± (T_9x30.csv)**: æœˆã”ã¨ã®å‡ºè·å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
        - **å•†å“ãƒã‚¹ã‚¿ (PACK_Classification.csv)**: å•†å“ã®åˆ†é¡æƒ…å ±ï¼ˆå¤§åˆ†é¡ã€ä¸­åˆ†é¡ã€å°åˆ†é¡ï¼‰ã‚’æ ¼ç´ã€‚
        - **åœ¨åº«æƒ…å ± (CZ04003_*.csv)**: æ—¥ã€…ã®åœ¨åº«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€‚ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
        - **é€±é–“å‡ºè·æƒ…å ± (T_9x07.csv)**: é€±ã”ã¨ã®å‡ºè·å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
        """)

except Exception as e:
    logging.critical(f"--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœªè£œè¶³ã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} ---", exc_info=True)
    if "Image size" in str(e):
         st.error("ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ©ãƒ•è¤‡é›‘ã™ãã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶çµã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚")
         logging.error(f"ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒ©ãƒ¼ï¼ˆImage size limitï¼‰: {e}")
    else:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")






