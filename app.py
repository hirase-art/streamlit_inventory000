import streamlit as st
import pandas as pd
import logging
import glob # â˜… ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’æ‰±ã†ãŸã‚ã«globã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.pyplot as plt # â˜… ã‚°ãƒ©ãƒ•ä½œæˆã®ãŸã‚ã«pyplotã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import japanize_matplotlib # æ—¥æœ¬èªæ–‡å­—åŒ–ã‘å¯¾ç­–

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='app.log',
    filemode='w'
)

logging.info("--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---")

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data
def load_single_csv(path, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰å˜ä¸€ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    logging.info(f"load_single_csv: {path} ã‚’ {encoding} ã¨ã—ã¦èª­ã¿è¾¼ã¿é–‹å§‹ã€‚")
    try:
        # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
        df = pd.read_csv(path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str, 'æ¥­å‹™åŒºåˆ†ID': str})
        logging.info(f"load_single_csv: {path} ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return df
    except FileNotFoundError:
        logging.error(f"load_single_csv: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
        st.error(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ•ã‚¡ã‚¤ãƒ«ãŒã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„: {path}")
        return None
    except Exception as e:
        logging.error(f"load_single_csv: {path} èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data
def load_multiple_csv(pattern, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹è¤‡æ•°ã®CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆã™ã‚‹é–¢æ•°"""
    logging.info(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã®æ¤œç´¢ã‚’é–‹å§‹ã€‚")
    
    file_list = glob.glob(pattern)
    
    if not file_list:
        logging.warning(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
        st.warning(f"è­¦å‘Š: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {pattern}")
        return None

    logging.info(f"{len(file_list)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {file_list}")
    
    df_list = []
    for file_path in file_list:
        try:
            # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
            df = pd.read_csv(file_path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str})
            df_list.append(df)
        except Exception as e:
            logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {file_path}, ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            continue 
            
    if not df_list:
        logging.error("èª­ã¿è¾¼ã‚ãŸãƒ‡ãƒ¼ã‚¿ãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    logging.info("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã—ã¾ã™ã€‚")
    combined_df = pd.concat(df_list, ignore_index=True)
    return combined_df

try:
    st.set_page_config(layout="wide") # â˜…ãƒšãƒ¼ã‚¸ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ãƒ¯ã‚¤ãƒ‰ã«å¤‰æ›´
    st.title('ğŸ“Š åœ¨åº«ãƒ»å‡ºè·ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚¢ãƒ—ãƒª')

    # --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    DATA_PATH1 = "T_9x30.csv"
    DATA_PATH_MASTER = "PACK_Classification.csv"
    DATA_PATH3_PATTERN = "CZ04003_*.csv"
    DATA_PATH5 = "T_9x07.csv"

    df1 = load_single_csv(DATA_PATH1, encoding='utf-8')
    df_master = load_single_csv(DATA_PATH_MASTER, encoding='utf-8') 
    df3 = load_multiple_csv(DATA_PATH3_PATTERN, encoding='cp932')
    df5 = load_single_csv(DATA_PATH5, encoding='utf-8')

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å…ˆã«ã™ã¹ã¦å®šç¾© ---

    # --- å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ã®æº–å‚™ ---
    if df1 is not None and df_master is not None:
        master_cols = ['å•†å“ID', 'å•†å“å', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        if all(col in df_master.columns for col in master_cols):
            df_master_shipping = df_master[master_cols].drop_duplicates(subset='å•†å“ID')
            base_df_monthly = pd.merge(df1, df_master_shipping, on='å•†å“ID', how='left')
            if df5 is not None:
                base_df_weekly = pd.merge(df5, df_master_shipping, on='å•†å“ID', how='left')
            
            st.sidebar.header(":blue[å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
            
            selected_daibunrui_shipping = "ã™ã¹ã¦"
            if 'å¤§åˆ†é¡' in base_df_monthly.columns:
                daibunrui_options = base_df_monthly['å¤§åˆ†é¡'].dropna().unique().tolist()
                daibunrui_options.sort()
                daibunrui_options.insert(0, "ã™ã¹ã¦")
                selected_daibunrui_shipping = st.sidebar.selectbox("å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿:", options=daibunrui_options, key='daibunrui_shipping')
            
            df_after_daibunrui_filter = base_df_monthly[base_df_monthly['å¤§åˆ†é¡'] == selected_daibunrui_shipping] if selected_daibunrui_shipping != "ã™ã¹ã¦" else base_df_monthly
            selected_shobunrui_shipping = []
            if 'å°åˆ†é¡' in df_after_daibunrui_filter.columns:
                shobunrui_options = df_after_daibunrui_filter['å°åˆ†é¡'].dropna().unique().tolist()
                shobunrui_options.sort()
                selected_shobunrui_shipping = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options, key='shobunrui_shipping')
            
            df_after_shobunrui_filter = df_after_daibunrui_filter[df_after_daibunrui_filter['å°åˆ†é¡'].isin(selected_shobunrui_shipping)] if selected_shobunrui_shipping else df_after_daibunrui_filter
            product_ids_input_shipping = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_shipping').strip()
            selected_product_ids_shipping = [pid.strip() for pid in product_ids_input_shipping.split(',')] if product_ids_input_shipping else []
            df_after_product_id_filter = df_after_shobunrui_filter[df_after_shobunrui_filter['å•†å“ID'].isin(selected_product_ids_shipping)] if selected_product_ids_shipping else df_after_shobunrui_filter

            gyomu_display_map = {'4': 'å¸å‡ºè·æ©Ÿèƒ½', '7': 'é€šè²©å‡ºè·æ©Ÿèƒ½'}
            selected_gyomu = "ã™ã¹ã¦"
            if 'æ¥­å‹™åŒºåˆ†ID' in df_after_product_id_filter.columns:
                gyomu_options = df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'].dropna().unique().tolist()
                gyomu_options.sort()
                gyomu_options.insert(0, "ã™ã¹ã¦")
                selected_gyomu = st.sidebar.radio("æ¥­å‹™åŒºåˆ†IDã§çµã‚Šè¾¼ã¿:", options=gyomu_options, key='gyomu_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else gyomu_display_map.get(x, x))
            
            df_after_gyomu_filter = df_after_product_id_filter[df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu] if selected_gyomu != "ã™ã¹ã¦" else df_after_product_id_filter
            soko_display_map = {'7': 'å¤§é˜ª', '8': 'åƒè‘‰'}
            selected_soko_shipping = "ã™ã¹ã¦"
            if 'å€‰åº«ID' in df_after_gyomu_filter.columns:
                soko_options = df_after_gyomu_filter['å€‰åº«ID'].dropna().unique().tolist()
                soko_options.sort()
                soko_options.insert(0, "ã™ã¹ã¦")
                selected_soko_shipping = st.sidebar.radio("å€‰åº«IDã§çµã‚Šè¾¼ã¿:", options=soko_options, key='soko_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else soko_display_map.get(x, x))

    # --- åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ã®æº–å‚™ ---
    if df3 is not None and df_master is not None:
        base_df_stock = df3.copy()
        master_cols_stock = ['å•†å“ID', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        if all(col in df_master.columns for col in master_cols_stock):
            cols_to_drop = ['å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
            df3_for_merge = df3.drop(columns=cols_to_drop, errors='ignore')
            df_master_stock = df_master[master_cols_stock].drop_duplicates(subset='å•†å“ID')
            base_df_stock = pd.merge(df3_for_merge, df_master_stock, on='å•†å“ID', how='left')
        
        st.sidebar.header(":blue[åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
        selected_daibunrui_stock = "ã™ã¹ã¦"
        if 'å¤§åˆ†é¡' in base_df_stock.columns:
            daibunrui_options_stock = base_df_stock['å¤§åˆ†é¡'].dropna().unique().tolist()
            daibunrui_options_stock.sort()
            daibunrui_options_stock.insert(0, "ã™ã¹ã¦")
            selected_daibunrui_stock = st.sidebar.selectbox("å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿:", options=daibunrui_options_stock, key='daibunrui_stock')
        
        df_after_daibunrui_filter_stock = base_df_stock[base_df_stock['å¤§åˆ†é¡'] == selected_daibunrui_stock] if selected_daibunrui_stock != "ã™ã¹ã¦" else base_df_stock
        selected_shobunrui_stock = []
        if 'å°åˆ†é¡' in df_after_daibunrui_filter_stock.columns:
            shobunrui_options_stock = df_after_daibunrui_filter_stock['å°åˆ†é¡'].dropna().unique().tolist()
            shobunrui_options_stock.sort()
            selected_shobunrui_stock = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options_stock, key='shobunrui_stock')
        
        df_after_shobunrui_filter_stock = df_after_daibunrui_filter_stock[df_after_daibunrui_filter_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock)] if selected_shobunrui_stock else df_after_daibunrui_filter_stock
        product_ids_input_stock = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_stock').strip()
        selected_product_ids_stock = [pid.strip() for pid in product_ids_input_stock.split(',')] if product_ids_input_stock else []
        df_after_product_id_filter_stock = df_after_shobunrui_filter_stock[df_after_shobunrui_filter_stock['å•†å“ID'].isin(selected_product_ids_stock)] if selected_product_ids_stock else df_after_shobunrui_filter_stock
        selected_quality_stock = "ã™ã¹ã¦"
        if 'å“è³ªåŒºåˆ†å' in df_after_product_id_filter_stock.columns:
            quality_options_stock = df_after_product_id_filter_stock['å“è³ªåŒºåˆ†å'].dropna().unique().tolist()
            quality_options_stock.insert(0, "ã™ã¹ã¦")
            selected_quality_stock = st.sidebar.radio("å“è³ªåŒºåˆ†åã§çµã‚Šè¾¼ã¿:", options=quality_options_stock, key='quality_stock')

    # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… ã“ã“ã‹ã‚‰ã‚¿ãƒ–ã‚’ä½œæˆ
    tab_stock, tab_shipping = st.tabs(["ğŸ“Š åœ¨åº«æƒ…å ±", "ğŸ“ å‡ºè·æƒ…å ±"])

    # --- åœ¨åº«æƒ…å ±ã®ã‚¿ãƒ– ---
    with tab_stock:
        st.header("ğŸ“¦ åœ¨åº«æƒ…å ±")
        if df3 is not None and df_master is not None:
            pivot_target_df_stock = base_df_stock[
                (base_df_stock['å¤§åˆ†é¡'] == selected_daibunrui_stock if selected_daibunrui_stock != "ã™ã¹ã¦" else True) &
                (base_df_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock) if selected_shobunrui_stock else True) &
                (base_df_stock['å•†å“ID'].isin(selected_product_ids_stock) if selected_product_ids_stock else True) &
                (base_df_stock['å“è³ªåŒºåˆ†å'] == selected_quality_stock if selected_quality_stock != "ã™ã¹ã¦" else True)
            ]

            st.markdown("---")
            st.subheader("åˆ©ç”¨å¯èƒ½åœ¨åº«")
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_stock}` | **å°åˆ†é¡:** `{selected_shobunrui_stock if selected_shobunrui_stock else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_stock if selected_product_ids_stock else 'ã™ã¹ã¦'}` | **å“è³ªåŒºåˆ†å:** `{selected_quality_stock}`")
            
            required_cols_stock = ["å•†å“ID", "å•†å“å", "å€‰åº«å", "åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)", "å¼•å½“æ•°"]
            if not pivot_target_df_stock.empty and all(col in pivot_target_df_stock.columns for col in required_cols_stock):
                try:
                    pivot_target_df_stock['å®Ÿåœ¨åº«æ•°'] = pd.to_numeric(pivot_target_df_stock['åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)'], errors='coerce').fillna(0) - pd.to_numeric(pivot_target_df_stock['å¼•å½“æ•°'], errors='coerce').fillna(0)
                    pivot_index_stock = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"]
                    pivot_stock = pivot_target_df_stock.pivot_table(index=pivot_index_stock, columns="å€‰åº«å", values="å®Ÿåœ¨åº«æ•°", aggfunc="sum").fillna(0)
                    pivot_stock_filtered = pivot_stock[pivot_stock.sum(axis=1) != 0]

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info("ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯åˆè¨ˆåœ¨åº«ãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_stock_filtered.reset_index(), height=400, use_container_width=True)
                    with col2:
                        st.write("ã‚°ãƒ©ãƒ•ï¼ˆå¤§åˆ†é¡åˆ¥ åœ¨åº«æ§‹æˆæ¯”ï¼‰")
                        if 'å¤§åˆ†é¡' in pivot_target_df_stock.columns:
                            pie_data = pivot_target_df_stock.groupby('å¤§åˆ†é¡')['å®Ÿåœ¨åº«æ•°'].sum()
                            pie_data = pie_data[pie_data > 0] 
                            if not pie_data.empty:
                                fig, ax = plt.subplots()
                                ax.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10}) 
                                ax.axis('equal')
                                st.pyplot(fig)
                            else:
                                st.warning("ã‚°ãƒ©ãƒ•åŒ–ã§ãã‚‹åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                except Exception as e:
                    st.error(f"ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã¾ãŸã¯ã‚°ãƒ©ãƒ•ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            else:
                st.warning("åœ¨åº«æƒ…å ±: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

    # --- å‡ºè·æƒ…å ±ã®ã‚¿ãƒ– ---
    with tab_shipping:
        st.header("ğŸšš å‡ºè·æƒ…å ±")
        if df1 is not None and df_master is not None:
            # æœˆé–“å‡ºè·
            st.markdown("---")
            st.subheader("æœˆé–“å‡ºè·æ•°")
            gyomu_display_str = "ã™ã¹ã¦" if selected_gyomu == "ã™ã¹ã¦" else gyomu_display_map.get(selected_gyomu, selected_gyomu)
            soko_display_str = "ã™ã¹ã¦" if selected_soko_shipping == "ã™ã¹ã¦" else soko_display_map.get(selected_soko_shipping, selected_soko_shipping)
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
            
            df_monthly_filtered = base_df_monthly[
                (base_df_monthly['å¤§åˆ†é¡'] == selected_daibunrui_shipping if selected_daibunrui_shipping != "ã™ã¹ã¦" else True) &
                (base_df_monthly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                (base_df_monthly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                (base_df_monthly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                (base_df_monthly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
            ]
            
            required_cols = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "month_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
            if not df_monthly_filtered.empty and all(col in df_monthly_filtered.columns for col in required_cols):
                pivot = df_monthly_filtered.pivot_table(index=["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"], columns="month_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                recent_cols = pivot.columns[-12:]
                pivot_filtered = pivot[pivot[recent_cols].sum(axis=1) != 0]
                pivot_display = pivot_filtered.loc[:, recent_cols]

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.info("ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘12ãƒ¶æœˆåˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                    st.dataframe(pivot_display.reset_index(), height=400, use_container_width=True)
                with col2:
                    st.write("ã‚°ãƒ©ãƒ•ï¼ˆå•†å“åˆ¥ç©ã¿ä¸Šã’ï¼‰")
                    chart_df_monthly = df_monthly_filtered.pivot_table(index='month_code', columns='å•†å“å', values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                    st.bar_chart(chart_df_monthly.iloc[-12:, :]) 
            else:
                st.warning("æœˆé–“å‡ºè·: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

            # é€±é–“å‡ºè·
            if df5 is not None:
                st.markdown("---")
                st.subheader("é€±é–“å‡ºè·æ•°")
                st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
                
                df_weekly_filtered = base_df_weekly[
                    (base_df_weekly['å¤§åˆ†é¡'] == selected_daibunrui_shipping if selected_daibunrui_shipping != "ã™ã¹ã¦" else True) &
                    (base_df_weekly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                    (base_df_weekly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                    (base_df_weekly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                    (base_df_weekly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
                ]
                
                required_cols_weekly = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "week_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
                if not df_weekly_filtered.empty and all(col in df_weekly_filtered.columns for col in required_cols_weekly):
                    pivot_weekly = df_weekly_filtered.pivot_table(index=["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"], columns="week_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                    recent_cols_weekly = pivot_weekly.columns[-12:]
                    pivot_weekly_filtered = pivot_weekly[pivot_weekly[recent_cols_weekly].sum(axis=1) != 0]
                    pivot_weekly_display = pivot_weekly_filtered.loc[:, recent_cols_weekly]

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info("ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘12é€±åˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_weekly_display.reset_index(), height=400, use_container_width=True)
                    with col2:
                        st.write("ã‚°ãƒ©ãƒ•ï¼ˆå•†å“åˆ¥ç©ã¿ä¸Šã’ï¼‰")
                        chart_df_weekly = df_weekly_filtered.pivot_table(index='week_code', columns='å•†å“å', values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                        st.bar_chart(chart_df_weekly.iloc[-12:, :])
                else:
                    st.warning("é€±é–“å‡ºè·: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

    # --- å…±é€šã®ãƒ•ãƒƒã‚¿ãƒ¼ãªã© ---
    st.markdown("---")
    with st.expander("å–ã‚Šè¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä»•æ§˜ã«ã¤ã„ã¦"):
        st.markdown("""
        - **æœˆé–“å‡ºè·æƒ…å ± (T_9x30.csv)**: æœˆã”ã¨ã®å‡ºè·å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
        - **å•†å“ãƒã‚¹ã‚¿ (PACK_Classification.csv)**: å•†å“ã®åˆ†é¡æƒ…å ±ï¼ˆå¤§åˆ†é¡ã€ä¸­åˆ†é¡ã€å°åˆ†é¡ï¼‰ã‚’æ ¼ç´ã€‚
        - **åœ¨åº«æƒ…å ± (CZ04003_*.csv)**: æ—¥ã€…ã®åœ¨åº«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€‚ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
        - **é€±é–“å‡ºè·æƒ…å ± (T_9x07.csv)**: é€±ã”ã¨ã®å‡ºè·å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
        """)

except Exception as e:
    logging.critical(f"--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœªè£œè¶³ã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} ---", exc_info=True)
    st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
