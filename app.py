import streamlit as st
import pandas as pd
import logging
import glob # â˜… ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚’æ‰±ã†ãŸã‚ã«globã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.pyplot as plt # â˜… ã‚°ãƒ©ãƒ•ä½œæˆã®ãŸã‚ã«pyplotã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import japanize_matplotlib # æ—¥æœ¬èªæ–‡å­—åŒ–ã‘å¯¾ç­–
import numpy as np # â˜… æ•°å€¤è¨ˆç®—ã®ãŸã‚ã«numpyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import matplotlib.gridspec as gridspec # â˜… ã‚°ãƒ©ãƒ•ã‚¨ãƒªã‚¢åˆ†å‰²ã®ãŸã‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='app.log',
    filemode='w'
)

logging.info("--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---")

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data
def load_single_csv(path, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰å˜ä¸€ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    logging.info(f"load_single_csv: {path} ã‚’ {encoding} ã¨ã—ã¦èª­ã¿è¾¼ã¿é–‹å§‹ã€‚")
    try:
        # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
        df = pd.read_csv(path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str, 'æ¥­å‹™åŒºåˆ†ID': str})
        logging.info(f"load_single_csv: {path} ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return df
    except FileNotFoundError:
        logging.error(f"load_single_csv: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
        st.error(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ•ã‚¡ã‚¤ãƒ«ãŒã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„: {path}")
        return None
    except Exception as e:
        logging.error(f"load_single_csv: {path} èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data
def load_multiple_csv(pattern, encoding='utf-8'):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹è¤‡æ•°ã®CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆã™ã‚‹é–¢æ•°"""
    logging.info(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã®æ¤œç´¢ã‚’é–‹å§‹ã€‚")
    
    file_list = glob.glob(pattern)
    
    if not file_list:
        logging.warning(f"load_multiple_csv: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
        st.warning(f"è­¦å‘Š: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {pattern}")
        return None

    logging.info(f"{len(file_list)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {file_list}")
    
    df_list = []
    for file_path in file_list:
        try:
            # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã‚ˆã†ã«æŒ‡å®š
            df = pd.read_csv(file_path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str})
            df_list.append(df)
        except Exception as e:
            logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {file_path}, ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            continue 
            
    if not df_list:
        logging.error("èª­ã¿è¾¼ã‚ãŸãƒ‡ãƒ¼ã‚¿ãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    logging.info("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã—ã¾ã™ã€‚")
    combined_df = pd.concat(df_list, ignore_index=True)
    return combined_df

# æ£’ã‚°ãƒ©ãƒ•ã«æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°
def add_labels_to_stacked_bar(ax, data_df):
    """ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ã®å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã™ã‚‹"""
    bottom = pd.Series([0.0] * len(data_df), index=data_df.index) 
    x_positions = np.arange(len(data_df.index)) 

    for col in data_df.columns:
        values = data_df[col]
        threshold = values.sum() * 0.03 
        non_zero_values = values[values > threshold] 
        
        valid_indices = non_zero_values.index
        y_pos = bottom.loc[valid_indices].fillna(0) + non_zero_values.fillna(0) / 2
        
        x_pos_map = {label: i for i, label in enumerate(data_df.index)}
        valid_x_positions = [x_pos_map.get(idx) for idx in valid_indices if x_pos_map.get(idx) is not None]

        valid_non_zero_values = non_zero_values.iloc[:len(valid_x_positions)]
        valid_y_pos = y_pos.iloc[:len(valid_x_positions)]

        for i, val in enumerate(valid_non_zero_values):
             if i < len(valid_x_positions):
                ax.text(valid_x_positions[i], valid_y_pos.iloc[i], f'{int(val)}', 
                        ha='center', va='center', fontsize=5, color='white', fontweight='bold') 
            
        bottom += values.fillna(0) 

try:
    st.set_page_config(layout="wide") 
    st.title('ğŸ“Š åœ¨åº«ãƒ»å‡ºè·ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚¢ãƒ—ãƒª')

    # --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    DATA_PATH1 = "T_9x30.csv"
    DATA_PATH_MASTER = "PACK_Classification.csv"
    DATA_PATH3_PATTERN = "CZ04003_*.csv"
    DATA_PATH5 = "T_9x07.csv"

    df1 = load_single_csv(DATA_PATH1, encoding='utf-8')
    df_master = load_single_csv(DATA_PATH_MASTER, encoding='utf-8') 
    df3 = load_multiple_csv(DATA_PATH3_PATTERN, encoding='cp932')
    df5 = load_single_csv(DATA_PATH5, encoding='utf-8')


    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å…ˆã«ã™ã¹ã¦å®šç¾© ---
    base_df_monthly = pd.DataFrame()
    base_df_weekly = pd.DataFrame()
    selected_daibunrui_shipping = "ã™ã¹ã¦"
    selected_shobunrui_shipping = []
    selected_product_ids_shipping = []
    selected_gyomu = "ã™ã¹ã¦"
    selected_soko_shipping = "ã™ã¹ã¦"
    gyomu_display_map = {'4': 'å¸å‡ºè·æ©Ÿèƒ½', '7': 'é€šè²©å‡ºè·æ©Ÿèƒ½'}
    soko_display_map = {'7': 'å¤§é˜ª', '8': 'åƒè‘‰'}
    num_months = 12 
    num_weeks = 12 

    if df1 is not None and df_master is not None:
        master_cols = ['å•†å“ID', 'å•†å“å', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        if all(col in df_master.columns for col in master_cols):
            df_master_shipping = df_master[master_cols].drop_duplicates(subset='å•†å“ID')
            base_df_monthly = pd.merge(df1, df_master_shipping, on='å•†å“ID', how='left')
            if df5 is not None:
                base_df_weekly = pd.merge(df5, df_master_shipping, on='å•†å“ID', how='left')
            
            st.sidebar.header(":blue[å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
            
            if 'å¤§åˆ†é¡' in base_df_monthly.columns:
                daibunrui_options = base_df_monthly['å¤§åˆ†é¡'].dropna().unique().tolist()
                daibunrui_options.sort()
                daibunrui_options.insert(0, "ã™ã¹ã¦")
                selected_daibunrui_shipping = st.sidebar.selectbox("å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿:", options=daibunrui_options, key='daibunrui_shipping')
            
            df_after_daibunrui_filter = base_df_monthly[base_df_monthly['å¤§åˆ†é¡'] == selected_daibunrui_shipping] if selected_daibunrui_shipping != "ã™ã¹ã¦" else base_df_monthly
            if 'å°åˆ†é¡' in df_after_daibunrui_filter.columns:
                shobunrui_options = df_after_daibunrui_filter['å°åˆ†é¡'].dropna().unique().tolist()
                shobunrui_options.sort()
                selected_shobunrui_shipping = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options, key='shobunrui_shipping')
            
            df_after_shobunrui_filter = df_after_daibunrui_filter[df_after_daibunrui_filter['å°åˆ†é¡'].isin(selected_shobunrui_shipping)] if selected_shobunrui_shipping else df_after_daibunrui_filter
            product_ids_input_shipping = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_shipping').strip()
            selected_product_ids_shipping = [pid.strip() for pid in product_ids_input_shipping.split(',')] if product_ids_input_shipping else []
            df_after_product_id_filter = df_after_shobunrui_filter[df_after_shobunrui_filter['å•†å“ID'].isin(selected_product_ids_shipping)] if selected_product_ids_shipping else df_after_shobunrui_filter

            if 'æ¥­å‹™åŒºåˆ†ID' in df_after_product_id_filter.columns:
                gyomu_options = df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'].dropna().unique().tolist()
                gyomu_options.sort()
                gyomu_options.insert(0, "ã™ã¹ã¦")
                selected_gyomu = st.sidebar.radio("æ¥­å‹™åŒºåˆ†IDã§çµã‚Šè¾¼ã¿:", options=gyomu_options, key='gyomu_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else gyomu_display_map.get(x, x))
            
            df_after_gyomu_filter = df_after_product_id_filter[df_after_product_id_filter['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu] if selected_gyomu != "ã™ã¹ã¦" else df_after_product_id_filter
            if 'å€‰åº«ID' in df_after_gyomu_filter.columns:
                soko_options = df_after_gyomu_filter['å€‰åº«ID'].dropna().unique().tolist()
                soko_options.sort()
                soko_options.insert(0, "ã™ã¹ã¦")
                selected_soko_shipping = st.sidebar.radio("å€‰åº«IDã§çµã‚Šè¾¼ã¿:", options=soko_options, key='soko_shipping', format_func=lambda x: "ã™ã¹ã¦" if x == "ã™ã¹ã¦" else soko_display_map.get(x, x))
            
            st.sidebar.markdown("---")
            num_months = st.sidebar.slider("æœˆé–“è¡¨ç¤ºæœŸé–“ï¼ˆãƒ¶æœˆï¼‰", min_value=3, max_value=15, value=12, key='num_months')
            num_weeks = st.sidebar.slider("é€±é–“è¡¨ç¤ºæœŸé–“ï¼ˆé€±ï¼‰", min_value=3, max_value=15, value=12, key='num_weeks')

    # --- åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ã®æº–å‚™ ---
    base_df_stock = pd.DataFrame()
    selected_daibunrui_stock = "ã™ã¹ã¦"
    selected_shobunrui_stock = []
    selected_product_ids_stock = []
    selected_quality_stock = "ã™ã¹ã¦"

    if df3 is not None and df_master is not None:
        master_cols_stock = ['å•†å“ID', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
        if all(col in df_master.columns for col in master_cols_stock):
            cols_to_drop = ['å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']
            df3_for_merge = df3.drop(columns=cols_to_drop, errors='ignore')
            df_master_stock = df_master[master_cols_stock].drop_duplicates(subset='å•†å“ID')
            base_df_stock = pd.merge(df3_for_merge, df_master_stock, on='å•†å“ID', how='left')
        
        st.sidebar.header(":blue[åœ¨åº«æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
        if 'å¤§åˆ†é¡' in base_df_stock.columns:
            daibunrui_options_stock = base_df_stock['å¤§åˆ†é¡'].dropna().unique().tolist()
            daibunrui_options_stock.sort()
            daibunrui_options_stock.insert(0, "ã™ã¹ã¦")
            selected_daibunrui_stock = st.sidebar.selectbox("å¤§åˆ†é¡ã§çµã‚Šè¾¼ã¿:", options=daibunrui_options_stock, key='daibunrui_stock')
        
        df_after_daibunrui_filter_stock = base_df_stock[base_df_stock['å¤§åˆ†é¡'] == selected_daibunrui_stock] if selected_daibunrui_stock != "ã™ã¹ã¦" else base_df_stock
        if 'å°åˆ†é¡' in df_after_daibunrui_filter_stock.columns:
            shobunrui_options_stock = df_after_daibunrui_filter_stock['å°åˆ†é¡'].dropna().unique().tolist()
            shobunrui_options_stock.sort()
            selected_shobunrui_stock = st.sidebar.multiselect("å°åˆ†é¡ã§çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰:", options=shobunrui_options_stock, key='shobunrui_stock')
        
        df_after_shobunrui_filter_stock = df_after_daibunrui_filter_stock[df_after_daibunrui_filter_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock)] if selected_shobunrui_stock else df_after_daibunrui_filter_stock
        product_ids_input_stock = st.sidebar.text_input("å•†å“IDã§çµã‚Šè¾¼ã¿ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å¯):", key='product_id_stock').strip()
        selected_product_ids_stock = [pid.strip() for pid in product_ids_input_stock.split(',')] if product_ids_input_stock else []
        df_after_product_id_filter_stock = df_after_shobunrui_filter_stock[df_after_shobunrui_filter_stock['å•†å“ID'].isin(selected_product_ids_stock)] if selected_product_ids_stock else df_after_shobunrui_filter_stock
        if 'å“è³ªåŒºåˆ†å' in df_after_product_id_filter_stock.columns:
            quality_options_stock = df_after_product_id_filter_stock['å“è³ªåŒºåˆ†å'].dropna().unique().tolist()
            quality_options_stock.insert(0, "ã™ã¹ã¦")
            selected_quality_stock = st.sidebar.radio("å“è³ªåŒºåˆ†åã§çµã‚Šè¾¼ã¿:", options=quality_options_stock, key='quality_stock')


    # --- ã‚¿ãƒ–ã®ä½œæˆ ---
    tab_shipping, tab_stock = st.tabs(["ğŸ“ å‡ºè·æƒ…å ±", "ğŸ“Š åœ¨åº«æƒ…å ±"])

    # --- å‡ºè·æƒ…å ±ã®ã‚¿ãƒ– ---
    with tab_shipping:
        st.header("ğŸšš å‡ºè·æƒ…å ±")
        if not base_df_monthly.empty:
            # æœˆé–“å‡ºè·
            st.markdown("---")
            st.subheader("æœˆé–“å‡ºè·æ•°")
            gyomu_display_str = "ã™ã¹ã¦" if selected_gyomu == "ã™ã¹ã¦" else gyomu_display_map.get(selected_gyomu, selected_gyomu)
            soko_display_str = "ã™ã¹ã¦" if selected_soko_shipping == "ã™ã¹ã¦" else soko_display_map.get(selected_soko_shipping, selected_soko_shipping)
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
            
            df_monthly_filtered = base_df_monthly[
                (base_df_monthly['å¤§åˆ†é¡'] == selected_daibunrui_shipping if selected_daibunrui_shipping != "ã™ã¹ã¦" else True) &
                (base_df_monthly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                (base_df_monthly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                (base_df_monthly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                (base_df_monthly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
            ]
            
            required_cols = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "month_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
            if not df_monthly_filtered.empty and all(col in df_monthly_filtered.columns for col in required_cols):
                pivot = df_monthly_filtered.pivot_table(index=["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"], columns="month_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                
                recent_cols = pivot.columns[-num_months:] 
                pivot_filtered = pivot[pivot[recent_cols].sum(axis=1) != 0]
                pivot_display = pivot_filtered.loc[:, recent_cols]

                col1, col2 = st.columns([2, 1])
                with col1:
                    st.info(f"ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘{num_months}ãƒ¶æœˆåˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                    st.dataframe(pivot_display.reset_index(), height=400, use_container_width=True)
                with col2:
                    st.write("ã‚°ãƒ©ãƒ•ï¼ˆå•†å“åˆ¥ç©ã¿ä¸Šã’ï¼‰") 
                    chart_df_monthly_base = df_monthly_filtered.pivot_table(index='month_code', columns='å•†å“å', values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                    chart_df_monthly_display = chart_df_monthly_base.iloc[-num_months:, :] 
                    
                    if not chart_df_monthly_display.empty:
                        chart_data_top = chart_df_monthly_display 

                        fig = plt.figure() 
                        # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… å‡¡ä¾‹ã‚¨ãƒªã‚¢ã®æ¯”ç‡ã‚’å°‘ã—å¢—ã‚„ã™
                        gs = gridspec.GridSpec(5, 1, height_ratios=[3, 1, 0.1, 0.1, 0.1]) # ä¸Š3/5, ä¸‹1/5 + ã‚¹ãƒšãƒ¼ã‚¹

                        ax_chart = fig.add_subplot(gs[0]) 
                        ax_legend = fig.add_subplot(gs[1]) 
                        ax_legend.axis('off') 

                        ax_chart.set_facecolor('lightgray') 

                        chart_data_top.plot(kind='bar', stacked=True, ax=ax_chart, legend=False) 
                        
                        try:
                           add_labels_to_stacked_bar(ax_chart, chart_data_top)
                        except Exception as label_e:
                            logging.warning(f"æœˆé–“ã‚°ãƒ©ãƒ•ã¸ã®ãƒ©ãƒ™ãƒ«è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼: {label_e}")
                            st.caption("æœˆé–“ã‚°ãƒ©ãƒ•ã¸ã®æ•°å€¤ãƒ©ãƒ™ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

                        ax_chart.set_xlabel("") 
                        ax_chart.set_ylabel("åˆè¨ˆå‡ºè·æ•°")
                        tick_interval = max(1, len(chart_data_top) // 10) 
                        ax_chart.set_xticks(np.arange(0, len(chart_data_top), tick_interval))
                        ax_chart.set_xticklabels(chart_data_top.index[::tick_interval], rotation=45, ha='right', fontsize=8) 
                        
                        handles, labels = ax_chart.get_legend_handles_labels()
                        ncol_legend = min(5, len(labels)) 
                        ax_legend.legend(handles, labels, title='å•†å“å', loc='upper center', ncol=ncol_legend, fontsize=6) 

                        # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… tight_layoutã«ä¸‹éƒ¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
                        plt.tight_layout(rect=[0, 0.05, 1, 1]) # rect=[left, bottom, right, top]
                        st.pyplot(fig, use_container_width=True)
                    else:
                        st.warning("æœˆé–“å‡ºè·ã‚°ãƒ©ãƒ•: è¡¨ç¤ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            else:
                st.warning("æœˆé–“å‡ºè·: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

            # é€±é–“å‡ºè·
            if not base_df_weekly.empty:
                st.markdown("---")
                st.subheader("é€±é–“å‡ºè·æ•°")
                st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_shipping}` | **å°åˆ†é¡:** `{selected_shobunrui_shipping if selected_shobunrui_shipping else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_shipping if selected_product_ids_shipping else 'ã™ã¹ã¦'}` | **æ¥­å‹™åŒºåˆ†ID:** `{gyomu_display_str}` | **å€‰åº«ID:** `{soko_display_str}`")
                
                df_weekly_filtered = base_df_weekly[
                    (base_df_weekly['å¤§åˆ†é¡'] == selected_daibunrui_shipping if selected_daibunrui_shipping != "ã™ã¹ã¦" else True) &
                    (base_df_weekly['å°åˆ†é¡'].isin(selected_shobunrui_shipping) if selected_shobunrui_shipping else True) &
                    (base_df_weekly['å•†å“ID'].isin(selected_product_ids_shipping) if selected_product_ids_shipping else True) &
                    (base_df_weekly['æ¥­å‹™åŒºåˆ†ID'] == selected_gyomu if selected_gyomu != "ã™ã¹ã¦" else True) &
                    (base_df_weekly['å€‰åº«ID'] == selected_soko_shipping if selected_soko_shipping != "ã™ã¹ã¦" else True)
                ]
                
                required_cols_weekly = ["å€‰åº«ID", "æ¥­å‹™åŒºåˆ†ID", "å•†å“ID", "week_code", "åˆè¨ˆå‡ºè·æ•°", "å•†å“å", "å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡"]
                if not df_weekly_filtered.empty and all(col in df_weekly_filtered.columns for col in required_cols_weekly):
                    pivot_weekly = df_weekly_filtered.pivot_table(index=["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"], columns="week_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
                    
                    recent_cols_weekly = pivot_weekly.columns[-num_weeks:]
                    pivot_weekly_filtered = pivot_weekly[pivot_weekly[recent_cols_weekly].sum(axis=1) != 0]
                    pivot_weekly_display = pivot_weekly_filtered.loc[:, recent_cols_weekly]

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info(f"ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ç›´è¿‘{num_weeks}é€±åˆè¨ˆãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_weekly_display.reset_index(), height=400, use_container_width=True)
                    with col2:
                        st.write("ã‚°ãƒ©ãƒ•ï¼ˆå•†å“åˆ¥ç©ã¿ä¸Šã’ï¼‰") 
                        chart_df_weekly_base = df_weekly_filtered.pivot_table(index='week_code', columns='å•†å“å', values='åˆè¨ˆå‡ºè·æ•°', aggfunc='sum').fillna(0)
                        chart_df_weekly_display = chart_df_weekly_base.iloc[-num_weeks:, :] 
                        
                        if not chart_df_weekly_display.empty:
                            chart_data_top_w = chart_df_weekly_display 

                            fig_w = plt.figure()
                            # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… å‡¡ä¾‹ã‚¨ãƒªã‚¢ã®æ¯”ç‡ã‚’å°‘ã—å¢—ã‚„ã™
                            gs_w = gridspec.GridSpec(5, 1, height_ratios=[3, 1, 0.1, 0.1, 0.1]) 

                            ax_chart_w = fig_w.add_subplot(gs_w[0])
                            ax_legend_w = fig_w.add_subplot(gs_w[1])
                            ax_legend_w.axis('off')

                            ax_chart_w.set_facecolor('lightgray')

                            chart_data_top_w.plot(kind='bar', stacked=True, ax=ax_chart_w, legend=False)
                            
                            try:
                                add_labels_to_stacked_bar(ax_chart_w, chart_data_top_w)
                            except Exception as label_e:
                                logging.warning(f"é€±é–“ã‚°ãƒ©ãƒ•ã¸ã®ãƒ©ãƒ™ãƒ«è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼: {label_e}")
                                st.caption("é€±é–“ã‚°ãƒ©ãƒ•ã¸ã®æ•°å€¤ãƒ©ãƒ™ãƒ«è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                            
                            ax_chart_w.set_xlabel("")
                            ax_chart_w.set_ylabel("åˆè¨ˆå‡ºè·æ•°")
                            tick_interval_w = max(1, len(chart_data_top_w) // 10) 
                            ax_chart_w.set_xticks(np.arange(0, len(chart_data_top_w), tick_interval_w))
                            ax_chart_w.set_xticklabels(chart_data_top_w.index[::tick_interval_w], rotation=45, ha='right', fontsize=8)
                            
                            handles_w, labels_w = ax_chart_w.get_legend_handles_labels()
                            ncol_legend_w = min(5, len(labels_w))
                            ax_legend_w.legend(handles_w, labels_w, title='å•†å“å', loc='upper center', ncol=ncol_legend_w, fontsize=6) 
                            
                            # â˜…â˜…â˜…ã€æ”¹ä¿®ãƒã‚¤ãƒ³ãƒˆã€‘â˜…â˜…â˜… tight_layoutã«ä¸‹éƒ¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
                            plt.tight_layout(rect=[0, 0.05, 1, 1]) 
                            st.pyplot(fig_w, use_container_width=True) 
                        else:
                             st.warning("é€±é–“å‡ºè·ã‚°ãƒ©ãƒ•: è¡¨ç¤ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.warning("é€±é–“å‡ºè·: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
    
    # --- åœ¨åº«æƒ…å ±ã®ã‚¿ãƒ– ---
    with tab_stock:
        st.header("ğŸ“¦ åœ¨åº«æƒ…å ±")
        if not base_df_stock.empty:
            pivot_target_df_stock = base_df_stock[
                (base_df_stock['å¤§åˆ†é¡'] == selected_daibunrui_stock if selected_daibunrui_stock != "ã™ã¹ã¦" else True) &
                (base_df_stock['å°åˆ†é¡'].isin(selected_shobunrui_stock) if selected_shobunrui_stock else True) &
                (base_df_stock['å•†å“ID'].isin(selected_product_ids_stock) if selected_product_ids_stock else True) &
                (base_df_stock['å“è³ªåŒºåˆ†å'] == selected_quality_stock if selected_quality_stock != "ã™ã¹ã¦" else True)
            ]

            st.markdown("---")
            st.subheader("åˆ©ç”¨å¯èƒ½åœ¨åº«")
            st.write(f"**å¤§åˆ†é¡:** `{selected_daibunrui_stock}` | **å°åˆ†é¡:** `{selected_shobunrui_stock if selected_shobunrui_stock else 'ã™ã¹ã¦'}` | **å•†å“ID:** `{selected_product_ids_stock if selected_product_ids_stock else 'ã™ã¹ã¦'}` | **å“è³ªåŒºåˆ†å:** `{selected_quality_stock}`")
            
            required_cols_stock = ["å•†å“ID", "å•†å“å", "å€‰åº«å", "åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)", "å¼•å½“æ•°"]
            if not pivot_target_df_stock.empty and all(col in pivot_target_df_stock.columns for col in required_cols_stock):
                try:
                    pivot_target_df_stock['å®Ÿåœ¨åº«æ•°'] = pd.to_numeric(pivot_target_df_stock['åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)'], errors='coerce').fillna(0) - pd.to_numeric(pivot_target_df_stock['å¼•å½“æ•°'], errors='coerce').fillna(0)
                    pivot_index_stock = ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID", "å•†å“å"]
                    pivot_stock = pivot_target_df_stock.pivot_table(index=pivot_index_stock, columns="å€‰åº«å", values="å®Ÿåœ¨åº«æ•°", aggfunc="sum").fillna(0)
                    pivot_stock_filtered = pivot_stock[pivot_stock.sum(axis=1) != 0]

                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.info("ãƒ’ãƒ³ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ã¯åˆè¨ˆåœ¨åº«ãŒ0ã§ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
                        st.dataframe(pivot_stock_filtered.reset_index(), height=400, use_container_width=True)
                    with col2:
                        st.write("ã‚°ãƒ©ãƒ•ï¼ˆå¤§åˆ†é¡åˆ¥ åœ¨åº«æ§‹æˆæ¯”ï¼‰")
                        if 'å¤§åˆ†é¡' in pivot_target_df_stock.columns:
                            pie_data = pivot_target_df_stock.groupby('å¤§åˆ†é¡')['å®Ÿåœ¨åº«æ•°'].sum()
                            pie_data = pie_data[pie_data > 0] 
                            if not pie_data.empty:
                                fig, ax = plt.subplots() 
                                ax.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 8}) 
                                ax.axis('equal')
                                fig.patch.set_facecolor('lightgray')
                                st.pyplot(fig, use_container_width=True) 
                            else:
                                st.warning("ã‚°ãƒ©ãƒ•åŒ–ã§ãã‚‹åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                except Exception as e:
                    st.error(f"ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã¾ãŸã¯ã‚°ãƒ©ãƒ•ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            else:
                st.warning("åœ¨åº«æƒ…å ±: é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã€å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

    # --- å…±é€šã®ãƒ•ãƒƒã‚¿ãƒ¼ãªã© ---
    # (çœç•¥)

except Exception as e:
    logging.critical(f"--- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœªè£œè¶³ã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} ---", exc_info=True)
    if "Image size" in str(e):
         st.error("ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ©ãƒ•ãŒè¤‡é›‘ã™ãã‚‹ãŸã‚ã€è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’çµã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚")
         logging.error(f"ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒ©ãƒ¼ï¼ˆImage size limitï¼‰: {e}")
    else:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

