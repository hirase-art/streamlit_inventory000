import streamlit as st
import pandas as pd
import logging
import glob
import matplotlib.pyplot as plt
import japanize_matplotlib
import numpy as np
import matplotlib.gridspec as gridspec

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(layout="wide", page_title="åœ¨åº«ãƒ»å‡ºè·å¯è¦–åŒ–ã‚¢ãƒ—ãƒª")

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='app.log',
    filemode='w'
)

# --------------------------------------------------------------------------
# 1. Supabase æ¥ç¶šè¨­å®š
# --------------------------------------------------------------------------
try:
    conn = st.connection("postgresql", type="sql")
except Exception as e:
    st.error(f"Supabaseæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

# --------------------------------------------------------------------------
# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# --------------------------------------------------------------------------

@st.cache_data(ttl=3600)
def load_data_from_supabase(table_name):
    """Supabaseã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å‹å¤‰æ›ã‚’è¡Œã†"""
    try:
        query = f'SELECT * FROM "{table_name}";'
        df = conn.query(query)
        # IDé–¢é€£ã®åˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ› (CSVèª­ã¿è¾¼ã¿æ™‚ã®æŒ™å‹•ã‚’å†ç¾)
        str_cols = ['å•†å“ID', 'å€‰åº«ID', 'æ¥­å‹™åŒºåˆ†ID', 'SET_ID', 'å“è³ªåŒºåˆ†ID']
        for col in str_cols:
            if col in df.columns:
                df[col] = df[col].astype(str).replace('None', np.nan).replace('nan', np.nan)
        return df
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®å–å¾—å¤±æ•—: {e}")
        return pd.DataFrame()

@st.cache_data
def load_single_csv(path, encoding='utf-8'):
    """ãƒã‚¹ã‚¿é¡ï¼ˆã¾ã GitHubã«ã‚ã‚‹ã‚‚ã®ï¼‰ã®èª­ã¿è¾¼ã¿"""
    try:
        return pd.read_csv(path, encoding=encoding, dtype={'å•†å“ID': str, 'å€‰åº«ID': str, 'æ¥­å‹™åŒºåˆ†ID': str, 'SET_ID': str})
    except:
        return None

# è£œåŠ©é–¢æ•°ï¼šæ£’ã‚°ãƒ©ãƒ•ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
def add_labels_to_stacked_bar(ax, data_df):
    try:
        bottom = pd.Series([0.0] * len(data_df), index=data_df.index)
        for col in data_df.columns:
            values = data_df[col].fillna(0)
            y_pos = bottom + values / 2
            for i, val in enumerate(values):
                if val > (data_df.sum(axis=1).max() * 0.05): # å°ã•ã™ãã‚‹å€¤ã¯éè¡¨ç¤º
                    ax.text(i, y_pos.iloc[i], f'{int(val)}', ha='center', va='center', fontsize=6, color='white', fontweight='bold')
            bottom += values
    except:
        pass

@st.cache_data
def convert_df(df):
    return df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')

# --------------------------------------------------------------------------
# 3. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# --------------------------------------------------------------------------

st.title('ğŸ“Š åœ¨åº«ãƒ»å‡ºè·ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚¢ãƒ—ãƒª')

# --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
# ãƒã‚¹ã‚¿é¡ã¯GitHubã‹ã‚‰èª­ã¿è¾¼ã¿
df1 = load_single_csv("T_9x30.csv", encoding='utf-8')
df_pack_master = load_single_csv("PACK_Classification.csv", encoding='utf-8')
df_set_master = load_single_csv("SET_Class.csv", encoding='utf-8')

# â˜… åœ¨åº«ã¨é€±é–“å‡ºè·ã¯ Supabase ã‹ã‚‰ç›´æ¥å–å¾— (GitHubã®CSVã¯ä¸è¦ã«ï¼)
with st.spinner('Supabaseã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸä¸­...'):
    df3 = load_data_from_supabase("åœ¨åº«æƒ…å ±") # å…ƒã® CZ04003_*.csv
    df5 = load_data_from_supabase("T_9x07")   # å…ƒã® T_9x07.csv

# --------------------------------------------------------------------------
# 4. ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»¥å‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨å¾©æ—§ï¼‰
# --------------------------------------------------------------------------
st.sidebar.header(":blue[å‡ºè·æƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿]")
unit_selection = st.sidebar.radio("é›†è¨ˆå˜ä½:", ["Pack", "SET"], horizontal=True)

# ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åˆ‡ã‚Šæ›¿ãˆ
if unit_selection == "Pack":
    df_master_shipping = df_pack_master.copy() if df_pack_master is not None else pd.DataFrame()
else:
    df_master_shipping = df_set_master.copy().rename(columns={'SET_ID': 'å•†å“ID', 'ã‚»ãƒƒãƒˆæ§‹æˆåç§°': 'å•†å“å'}) if df_set_master is not None else pd.DataFrame()

# åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
base_df_monthly = pd.merge(df1, df_master_shipping, on='å•†å“ID', how='left') if df1 is not None else pd.DataFrame()
base_df_weekly = pd.merge(df5, df_master_shipping, on='å•†å“ID', how='left') if not df5.empty else pd.DataFrame()

# å…±é€šãƒ•ã‚£ãƒ«ã‚¿
aggregation_level = st.sidebar.radio("é›†è¨ˆç²’åº¦:", ["å¤§åˆ†é¡", "ä¸­åˆ†é¡", "å°åˆ†é¡", "å•†å“ID"], index=3, horizontal=True)
show_total = st.sidebar.radio("åˆè¨ˆè¡¨ç¤º:", ["ãªã—", "ã‚ã‚Š"], horizontal=True)

# çµã‚Šè¾¼ã¿ UI
selected_daibunrui = st.sidebar.multiselect("å¤§åˆ†é¡:", options=sorted(base_df_monthly['å¤§åˆ†é¡'].dropna().unique().tolist()) if 'å¤§åˆ†é¡' in base_df_monthly.columns else [])
product_name_search = st.sidebar.text_input("å•†å“åæ¤œç´¢:").strip()

# --------------------------------------------------------------------------
# 5. ã‚¿ãƒ–è¡¨ç¤ºï¼ˆå‡ºè·ãƒ»åœ¨åº«ï¼‰
# --------------------------------------------------------------------------
tab_shipping, tab_stock = st.tabs(["ğŸ“ å‡ºè·æƒ…å ±", "ğŸ“Š åœ¨åº«æƒ…å ±"])

# --- å‡ºè·æƒ…å ±ã®ã‚¿ãƒ– ---
with tab_shipping:
    st.header("ğŸšš å‡ºè·æƒ…å ±")
    if not base_df_monthly.empty:
        # æœˆé–“å‡ºè·ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ”ãƒœãƒƒãƒˆ
        df_m_filtered = base_df_monthly[base_df_monthly['å¤§åˆ†é¡'].isin(selected_daibunrui)] if selected_daibunrui else base_df_monthly
        if product_name_search:
            df_m_filtered = df_m_filtered[df_m_filtered['å•†å“å'].str.contains(product_name_search, na=False)]

        # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        pivot_m = df_m_filtered.pivot_table(index=["å¤§åˆ†é¡", "å•†å“ID", "å•†å“å"], columns="month_code", values="åˆè¨ˆå‡ºè·æ•°", aggfunc="sum").fillna(0)
        st.subheader("æœˆé–“å‡ºè·æ•°ï¼ˆç›´è¿‘12ãƒ¶æœˆï¼‰")
        st.dataframe(pivot_m.tail(12))

# --- åœ¨åº«æƒ…å ±ã®ã‚¿ãƒ– ---
with tab_stock:
    st.header("ğŸ“¦ åœ¨åº«æƒ…å ±")
    if not df3.empty and df_pack_master is not None:
        # åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚¹ã‚¿ã®çµåˆ
        df3_master = pd.merge(df3, df_pack_master[['å•†å“ID', 'å¤§åˆ†é¡', 'ä¸­åˆ†é¡', 'å°åˆ†é¡']], on='å•†å“ID', how='left')
        
        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        df3_filtered = df3_master[df3_master['å¤§åˆ†é¡'].isin(selected_daibunrui)] if selected_daibunrui else df3_master
        
        col1, col2 = st.columns([2, 1])
        with col1:
            st.subheader("åœ¨åº«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df3_filtered.head(100))
            st.download_button("åœ¨åº«CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=convert_df(df3_filtered), file_name="inventory.csv")
        
        with col2:
            st.subheader("åœ¨åº«æ§‹æˆæ¯”ï¼ˆå¤§åˆ†é¡åˆ¥ï¼‰")
            if 'å¤§åˆ†é¡' in df3_filtered.columns:
                stock_pie = df3_filtered.groupby('å¤§åˆ†é¡')['åœ¨åº«æ•°(å¼•å½“æ•°ã‚’å«ã‚€)'].sum()
                fig, ax = plt.subplots()
                ax.pie(stock_pie, labels=stock_pie.index, autopct='%1.1f%%')
                st.pyplot(fig)

st.success("Supabaseã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã—ã¾ã—ãŸã€‚")
